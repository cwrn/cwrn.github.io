<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">

<title>Communicating with Robots <i>Naturally</i></title>
<link rel="stylesheet" type="text/css" href="./files/style.css">
<meta name="Author" content="Nakul Gopalan">
<meta name="description" content="">
<meta name="keywords" content="rss, language, robots, communication, grounding, learning">
</head>

<body>

<div id="container">

<div id="sidebar">

    <ul>
        <li><a href="#overview">Overview</a></li>
        <li><a href="#call">Call for Papers</a></li>
		<li><a href="#submissions">Submissions</a></li>
        <li><a href="#dates">Important Dates</a></li>
        <!--li><a href="#program">Program</a></li>
        <li><a href="#registration">Registration</a></li-->
        <li><a href="#organizers">Organizers</a></li>
    </ul>

</div>

<div class="section">
    <h2>R:SS 2018 Proposed Workshop (Proposed RSS Event) </h2>

    <!-- <h3>Part of <a href="http://www.sigdial.org/workshops/conference19/">SIGDIAL 2018</a></h3> -->
    <h3>June 29th or 30th, 2018<br></h3>

    <br>Carnegie Mellon University<br>
    <br>Pittsburgh, Pennsylvania, USA<br>
	
	<h4>Papers due on ***** at CMT</h4>

</div>

<div class="section"><a name="overview"></a>

    <h3>Overview</h3>

    <p>
        For robots to be our companions in workplaces and homes, we need them to understand the most common modalities of human communication, such as natural language and gestures. Moreover, to solve problems alongside humans, robots need to understand world models that humans express. These problems are complicated because human language is free-form with partially observable intentions, and most environments are unstructured and partially observable. Fortunately, language can be used both to instruct and to guide robots. In particular, natural language provides a common interface between mental models of the world. By learning and leveraging these models, robots can become more effective partners for humans when operating alongside them in shared environments.</p>

        <a href="mailto:webmaster@example.com">Jon Doe</a>

        <br> If you have any questions, please contact the <a href="mailto:rss_2018_workshop@protonmail.com">organizers</a>.<br>
	
</div>

<div class="section"><a name="call"></a>

    <h3>Call for Papers</h3>

    <p>Our objectives in this special session are to showcase recent and ongoing work on physically situated dialogue, and to identify paths forward in this space from research across communities including dialogue, robotics, computer vision, NLP, and AI. The special session will feature presentations, a poster session, and a panel discussion comprising a mix of experts in the topic area. We welcome submissions on any topic related to physically situated dialogue, including but not limited to:</p>
    <ul>
		<li>Interaction studies with smart-home devices</li>
		<li>Learning from demonstration through natural language dialogue</li>
		<li>Explainable AI in physical spaces</li>
		<li>Representations of physical surroundings / world modeling to support grounded communication</li>
		<li>Embodied visual question answering and/or generation</li>
		<li>Empirical studies of human-robot dialogue (Wizard-of-Oz based, simulated, or semi-autonomous)</li>
		<li>Computational models of dialogue management and/or turn-taking with physical agents</li>
		<li>Methods of building or leveraging common ground with physical agents in real-world or simulated environments</li>
		<li>Corpora of physically situated dialogue (Wizard-of-Oz based or otherwise)</li>
		<li>Multimodal information processing to support dialogue (including speech, gaze, gesture)</li>
		<li>Physical embodiment, voice, or personification of robots and their effects on human-robot dialogue</li>
		<li>Communicating feedback from robots using affordances in addition to speech</li>
		<li>Spoken language generation for physically situated dialogue</li>
    </ul>
    <p></p>
</div>
	
<div class="section"><a name="submissions"></a>
		
	<p>Researchers may choose to submit:</p>
	<ul>
	<li><b>Long papers and short papers</b> will present original research and go through the regular SIGdial peer review process by the general SIGdial program committee. These papers will appear in the main SIGdial proceedings and are presented with the main track. Long papers must be no longer than eight pages, including title, text, figures and tables, along with two additional pages for example discourses or dialogues and algorithms. Short papers should be no longer than four pages including title, text, figures and tables, along with one additional page for example discourses or dialogues and algorithms. An unlimited number of pages are allowed for references.</li>
	<li><b>Late-breaking and work-in-progress papers</b> will showcase ongoing work and focused, relevant contributions. Submissions need not present original work and are limited to four pages including references. These will be reviewed by the special session organizers and posted on the special session website. These papers will be presented as lightning talks or posters during the session. Authors will retain the copyright to their work so that they may submit to other venues as their work matures.</li>
	</ul>
		
	<h4>Long and short paper deadline: March 11 (<i>Final pdf due March 18</i>)</h4>
	<p>To submit a long or short paper, please go to the <a href="http://www.sigdial.org/workshops/conference19/">SIGDIAL 2018 main page</a> for conference submissions (deadline March 11). When submitting, indicate "Physically Situated Dialogue" as the candidate special session. All long and short submissions must follow the SIGDIAL 2018 format.</p>

	<h4>Late-breaking and work-in-progress paper deadline: April 28</h4>
	<p>To submit a late-breaking or work-in-progress paper, please email a 2-4 page PDF (including references) formatted using the SIGDIAL 2018 format guidelines, to: <a href="mailto:robodial@googlegroups.com">robodial@googlegroups.com</a> by April 28. You will receive a confirmation of your submission and notification before the Early Bird Registration deadline.</p>

</div>

<div class="section"><a name="dates"></a>
    <h3>Important Dates</h3>
    <table>
        <tbody><tr>
            <td>March 11, 2018</td>
            <td>Long and short paper initial submission deadline<br>
            (<a href="http://www.sigdial.org/workshops/conference19/">SIGDIAL submission system</a>)</td>
        </tr>
		<tr>
			<td>March 18, 2018</td>
			<td>Final pdf due for long and short paper submission</td>
        <tr>
            <td>April 20, 2018</td>
            <td>SIGDIAL notification of acceptance</td>
        </tr>
        <tr>
            <td>April 28, 2018</td>
            <td>Late-breaking and work-in-progress submission deadline<br>
            (Email directly to: <a href="mailto:robodial@googlegroups.com">robodial@googlegroups.com</a>)</td>
        </tr>
            <td>May 13, 2018</td>
            <td>Camera-ready submission deadline</td>
        </tr>
        <tr>
            <td>July 12-14, 2018</td>
            <td>SIGDIAL conference</td>
        </tr>
    </tbody></table>
</div>




<div class="section"><a name="organizers"></a>

    <h3>Organizers</h3>

    <p><a href="http://www.seanandrist.com">Sean Andrist</a>, Microsoft Research<br>
    <a href="https://users.soe.ucsc.edu/~slukin">Stephanie Lukin</a>, Army Research Lab<br>
    <a href="http://www.cs.cmu.edu/~mrmarge">Matthew Marge</a>, Army Research Lab<br>
    <a href="https://jessethomason.com">Jesse Thomason</a>, University of Texas at Austin<br>
    <a href="http://www.cs.cmu.edu/~zhouyu">Zhou Yu</a>, University of California, Davis</p>

</div>

	</div>
</body>
</html>